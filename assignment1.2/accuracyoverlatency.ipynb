{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f41dc98",
   "metadata": {
    "_cell_guid": "4cf02a6f-b7e9-4360-892d-b1a50793eb12",
    "_uuid": "2a1239f3-55fc-4dbb-9d3a-e90bfffa038c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-02-21T23:02:46.026846Z",
     "iopub.status.busy": "2025-02-21T23:02:46.026342Z",
     "iopub.status.idle": "2025-02-21T23:02:47.229936Z",
     "shell.execute_reply": "2025-02-21T23:02:47.228644Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.209974,
     "end_time": "2025-02-21T23:02:47.232887",
     "exception": false,
     "start_time": "2025-02-21T23:02:46.022913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: Enter any documentation that only people updating the metric should read here.\n",
    "\n",
    "All columns of the solution and submission dataframes are passed to your metric, except for the Usage column.\n",
    "\n",
    "Your metric must satisfy the following constraints:\n",
    "- You must have a function named score. Kaggle's evaluation system will call that function.\n",
    "- You can add your own arguments to score, but you cannot change the first three (solution, submission, and row_id_column_name).\n",
    "- All arguments for score must have type annotations.\n",
    "- score must return a single, finite, non-null float.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    # If you want an error message to be shown to participants, you must raise the error as a ParticipantVisibleError\n",
    "    # All other errors will only be shown to the competition host. This helps prevent unintentional leakage of solution data.\n",
    "    pass\n",
    "\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
    "    '''\n",
    "    Computes the accuracy/latency score.\n",
    "    '''\n",
    "    del solution[row_id_column_name]\n",
    "    del submission[row_id_column_name]\n",
    "    # TODO: adapt or remove this check depending on what data types make sense for your metric\n",
    "    for col in submission.columns:\n",
    "        if not pandas.api.types.is_numeric_dtype(submission[col]):\n",
    "            raise ParticipantVisibleError(f'Submission column {col} must be a number')\n",
    "    \n",
    "    # Compute metric here\n",
    "    acc_dict = (solution.iloc[:, 0] ==\n",
    "            submission.iloc[:, 0]).value_counts().to_dict()\n",
    "\n",
    "    accuracy = 100*(acc_dict[True] / len(solution))\n",
    "    latency = submission.iloc[:, 1].mean()\n",
    "    if latency < 0.01:\n",
    "        raise Exception(f'Latency should be in milliseconds. Your avg latency is {latency} and appears to be too low.')\n",
    "    score = accuracy / latency\n",
    "    return score\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5.169157,
   "end_time": "2025-02-21T23:02:47.857367",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-21T23:02:42.688210",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
